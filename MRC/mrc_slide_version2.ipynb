{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于滑动窗口策略的机器结合层次化建模阅读理解任务实现 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets transformers torch\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator, EvalPrediction\n",
    "from cmrc_eval import evaluate_cmrc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict.load_from_disk(\"mrc_data\")\n",
    "datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_document(context, max_length=512, overlap=128):\n",
    "    \"\"\"\n",
    "    将长文档分割成多个片段，每个片段的长度为max_length，片段之间有overlap的重叠。\n",
    "    \"\"\"\n",
    "    fragments = []\n",
    "    start = 0\n",
    "    while start < len(context):\n",
    "        end = min(start + max_length, len(context))\n",
    "        fragments.append(context[start:end])\n",
    "        if end == len(context):\n",
    "            break\n",
    "        start = end - overlap\n",
    "    return fragments\n",
    "\n",
    "def filter_relevant_fragments(question, fragments, top_k=3):\n",
    "    \"\"\"\n",
    "    根据问题与文档片段的相关性，筛选出最相关的top_k个片段。\n",
    "    \"\"\"\n",
    "    relevant_fragments = []\n",
    "    for fragment in fragments:\n",
    "        if any(keyword in fragment for keyword in question.split()):\n",
    "            relevant_fragments.append(fragment)\n",
    "    return relevant_fragments[:top_k]\n",
    "\n",
    "def process_func(examples):\n",
    "    tokenized_examples = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"offset_mapping\": [],\n",
    "        \"start_positions\": [],\n",
    "        \"end_positions\": [],\n",
    "        \"example_ids\": []\n",
    "    }\n",
    "    \n",
    "    for i, (question, context, answers) in enumerate(zip(examples[\"question\"], examples[\"context\"], examples[\"answers\"])):\n",
    "        fragments = split_document(context)\n",
    "        relevant_fragments = filter_relevant_fragments(question, fragments)\n",
    "        for fragment in relevant_fragments:\n",
    "            tokenized_fragment = tokenizer(\n",
    "                question,\n",
    "                fragment,\n",
    "                return_offsets_mapping=True,\n",
    "                max_length=384,\n",
    "                truncation=\"only_second\",\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            \n",
    "            # 处理答案位置\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "            context_start = tokenized_fragment.sequence_ids().index(1)\n",
    "            context_end = tokenized_fragment.sequence_ids().index(None, context_start) - 1\n",
    "            offset = tokenized_fragment[\"offset_mapping\"]\n",
    "            \n",
    "            if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "                start_token_pos = 0\n",
    "                end_token_pos = 0\n",
    "            else:\n",
    "                token_id = context_start\n",
    "                while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "                    token_id += 1\n",
    "                start_token_pos = token_id\n",
    "                token_id = context_end\n",
    "                while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "                    token_id -= 1\n",
    "                end_token_pos = token_id\n",
    "            \n",
    "            tokenized_examples[\"input_ids\"].append(tokenized_fragment[\"input_ids\"])\n",
    "            tokenized_examples[\"attention_mask\"].append(tokenized_fragment[\"attention_mask\"])\n",
    "            tokenized_examples[\"offset_mapping\"].append(tokenized_fragment[\"offset_mapping\"])\n",
    "            tokenized_examples[\"start_positions\"].append(start_token_pos)\n",
    "            tokenized_examples[\"end_positions\"].append(end_token_pos)\n",
    "            tokenized_examples[\"example_ids\"].append(examples[\"id\"][i])\n",
    "    \n",
    "    return tokenized_examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenied_datasets = datasets.map(process_func, batched=True, remove_columns=datasets[\"train\"].column_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 获取模型输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(start_logits, end_logits, examples, features):\n",
    "    predictions = {}\n",
    "    references = {}\n",
    "\n",
    "    example_to_feature = collections.defaultdict(list)\n",
    "    for idx, example_id in enumerate(features[\"example_ids\"]):\n",
    "        example_to_feature[example_id].append(idx)\n",
    "\n",
    "    n_best = 20 # 取前20个答案\n",
    "    max_answer_length = 30 # 答案最大长度\n",
    "\n",
    "    for example in examples:\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        for feature_idx in example_to_feature[example_id]:\n",
    "            start_logit = start_logits[feature_idx]\n",
    "            end_logit = end_logits[feature_idx]\n",
    "            offset = features[feature_idx][\"offset_mapping\"]\n",
    "            start_indexes = np.argsort(start_logit)[::-1][:n_best].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[::-1][:n_best].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if offset[start_index] is None or offset[end_index] is None:\n",
    "                        continue\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    answers.append({\n",
    "                        \"text\": context[offset[start_index][0]: offset[end_index][1]],\n",
    "                        \"score\": start_logit[start_index] + end_logit[end_index]\n",
    "                    })\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"score\"])\n",
    "            predictions[example_id] = best_answer[\"text\"]\n",
    "        else:\n",
    "            predictions[example_id] = \"\"\n",
    "        references[example_id] = example[\"answers\"][\"text\"]\n",
    "\n",
    "    return predictions, references\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metirc(pred):\n",
    "    start_logits, end_logits = pred[0]\n",
    "    if start_logits.shape[0] == len(tokenied_datasets[\"validation\"]):\n",
    "        p, r = get_result(start_logits, end_logits, datasets[\"validation\"], tokenied_datasets[\"validation\"])\n",
    "    else:\n",
    "        p, r = get_result(start_logits, end_logits, datasets[\"test\"], tokenied_datasets[\"test\"])\n",
    "    return evaluate_cmrc(p, r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"hfl/chinese-macbert-base\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 配置TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"models_for_qa\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=100\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 配置Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenied_datasets[\"train\"],\n",
    "    eval_dataset=tokenied_datasets[\"validation\"],\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    compute_metrics=metirc\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(question=\"小明在哪里上班？\", context=\"小明在北京上班\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
